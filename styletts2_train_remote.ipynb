{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset notes\n",
    "Training will fail with samples that are too short (<1 second)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download necessary files and set base config\n",
    "Specify the experiment name in `EXPERIMENT_NAME` and download the dataset and OOD text list. This will also configure some default settings in `Configs/config.yml`.\n",
    "\n",
    "`DATASET_URL` should point to a google drive hosted .zip containing:\n",
    "\n",
    "* train_list.txt\n",
    "* val_list.txt\n",
    "* *.wav\n",
    "\n",
    "`OOD_URL` should point to an OOD text list (the corresponding audio files are not needed; this is just used to test the TTS model on out-of-dataset text)\n",
    "\n",
    "`PRETRAINED`, if populated, should point to a direct download for a first-stage checkpoint or pretrained second-stage model (depending on what stage of training you plan to do.)\n",
    "\n",
    "Set `FIRST_STAGE` to True if the checkpoint you are supplying is a first stage checkpoint.\n",
    "\n",
    "It is recommended to use huggingface if your home internet bandwidth is not fantastic.\n",
    "If it is great, you can use `vastai copy` to copy files to the desired locations. These are the files you need to provide:\n",
    "* `/root/StyleTTS2/Data/train_list.txt`\n",
    "* `/root/StyleTTS2/Data/val_list.txt`\n",
    "* `/root/StyleTTS2/{EXPERIMENT_NAME}_data/*.wav`\n",
    "* `/root/StyleTTS2/Models/{EXPERIMENT_NAME}/pretrained.pth` (if using a pretrained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"Twilight0\"\n",
    "DATASET_URL = \"https://huggingface.co/datasets/therealvul/StyleTTS2MLP/resolve/main/StyleTTS2Omnidata.zip?download=true\"\n",
    "OOD_URL = \"https://gist.githubusercontent.com/effusiveperiscope/f3a6d48ad3463b63d1cb9a53bfab9dd9/raw/480f168c0f4cdc96c8863ac09c84978b364ee6a1/gistfile1.txt\"\n",
    "PRETRAINED_URL = \"https://huggingface.co/therealvul/StyleTTS2/resolve/main/Unfinished/omni_epoch_1st_00012.pth?download=true\"\n",
    "FIRST_STAGE = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset\n",
      "Downloaded dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting data:   0%|          | 2/6199 [00:00<05:22, 19.23file/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading OOD\n",
      "Downloading https://gist.githubusercontent.com/effusiveperiscope/f3a6d48ad3463b63d1cb9a53bfab9dd9/raw/480f168c0f4cdc96c8863ac09c84978b364ee6a1/gistfile1.txt to D:/styletts2test\\Data\\OOD_texts.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58.4k/58.4k [00:00<00:00, 8.07MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded OOD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg\n",
    "import os\n",
    "import gdown\n",
    "import urllib\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Download and extract dataset and data labels\n",
    "styletts_basedir = \"/root/StyleTTS2\" \n",
    "dataset_file = os.path.join(styletts_basedir, f\"dataset_{EXPERIMENT_NAME}.zip\")\n",
    "dataset_target_dir = os.path.join(styletts_basedir, f\"{EXPERIMENT_NAME}_data\")\n",
    "model_dir = os.path.join(styletts_basedir, \"Models\", EXPERIMENT_NAME)\n",
    "\n",
    "def download_file(url, dest):\n",
    "    print(f\"Downloading {url} to {dest}\")\n",
    "    with urllib.request.urlopen(url) as r, open(dest, 'wb') as out_file:\n",
    "        total_size = int(r.info().get('Content-Length', 0))\n",
    "        block_size = 1024\n",
    "        with tqdm(total=total_size, unit='B', unit_scale=True) as pbar:\n",
    "            while True:\n",
    "                data = r.read(block_size)\n",
    "                if not data:\n",
    "                    break\n",
    "                out_file.write(data)\n",
    "                pbar.update(len(data))\n",
    "\n",
    "print(\"Downloading dataset\")\n",
    "if not os.path.exists(dataset_file):\n",
    "    if \"drive.google.com\" in DATASET_URL:\n",
    "        gdown.download(DATASET_URL, output=dataset_file, quiet=False)\n",
    "    else:\n",
    "        download_file(DATASET_URL, dataset_file)\n",
    "print(\"Downloaded dataset\")\n",
    "\n",
    "assert(os.path.exists(dataset_file))\n",
    "\n",
    "if not os.path.exists(dataset_target_dir):\n",
    "    os.makedirs(dataset_target_dir, exist_ok=True)\n",
    "\n",
    "import zipfile\n",
    "with zipfile.ZipFile(dataset_file, 'r') as f:\n",
    "    files_count = len(f.namelist())\n",
    "    with tqdm(total=files_count, desc=f\"Extracting data\",\n",
    "        unit=\"file\") as pbar:\n",
    "        for info in f.namelist():\n",
    "            if info.endswith('.txt'):\n",
    "                unzip_file_path = os.path.join(styletts_basedir, 'Data')\n",
    "                f.extract(info, unzip_file_path) # Not sure if this can overwrite data?\n",
    "            elif info.endswith('.wav'):\n",
    "                unzip_file_path = dataset_target_dir\n",
    "                if os.path.exists(os.path.join(unzip_file_path, info)):\n",
    "                    continue\n",
    "                f.extract(info, unzip_file_path)\n",
    "            pbar.update(1)\n",
    "\n",
    "# Download OOD text\n",
    "ood_out_path = os.path.join(styletts_basedir, 'Data', 'OOD_texts.txt')\n",
    "if not os.path.exists(ood_out_path):\n",
    "    print(\"Downloading OOD\")\n",
    "    download_file(OOD_URL, ood_out_path)\n",
    "    print(\"Downloaded OOD\")\n",
    "\n",
    "# Download pretrained model\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "if FIRST_STAGE:\n",
    "    pretrain_name = 'epoch_1st_pretrained.pth'\n",
    "else:\n",
    "    pretrain_name = 'epoch_2nd_pretrained.pth'\n",
    "\n",
    "if PRETRAINED_URL is not None and not os.path.exists(os.path.join(model_dir, pretrain_name)):\n",
    "    print(\"Downloading pretrained model\")\n",
    "    download_file(PRETRAINED_URL, os.path.join(model_dir, pretrain_name))\n",
    "    print(\"Downloaded pretrained model\")\n",
    "\n",
    "# Setup base config\n",
    "import yaml\n",
    "config_path = os.path.join(styletts_basedir, 'Configs', 'config.yml')\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    config['log_dir'] = f'Models/{EXPERIMENT_NAME}'\n",
    "    if PRETRAINED_URL is not None:\n",
    "        config['pretrained_model'] = os.path.join(model_dir, pretrain_name)\n",
    "    config['data_params']['root_path'] = dataset_target_dir\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First stage (acoustic reconstruction training)\n",
    "This is only for training a model from scratch; you do not need to run this step if you only plan to finetune the diffusion (second stage) model.\n",
    "\n",
    "Configure the settings in the cell before running.\n",
    "\n",
    "* `TOTAL_EPOCHS` - total epochs used for first stage training. The paper used `100` epochs for LJSpeech (single speaker, 24hrs), `50` epochs for VCTK (109 speakers, 44hrs), and `30` epochs for LibriTTS (1151 speakers, 245hrs).\n",
    "* `TMA_START_POINT` - epochs before which transferable monotonic alignment is trained. Epochs in the TMA training regime are typically 20x longer than epochs before--so this should be expected to take up a large portion of training time. [The author used TMA_epoch = 5 for LibriTTS](https://github.com/yl4579/StyleTTS2/issues/18). I found `50` to be a reasonable value for a 5-hour dataset, so this can probably be lowered for larger datasets.\n",
    "* `PRETRAINED_MODEL` - if specified, loads the model and treats as a fresh run on the existing parameters (reset epoch/optimizer). Will be overridden by resume behavior.\n",
    "* `RESUME` - Whether to treat this run as resuming from an existing checkpoint (meaning that epoch numbers, optimizer states, etc. will not be reset). The default resume implementation will automatically select the most recent checkpoint from the current directory.\n",
    "* `EPOCHS_SAVE_FREQ` - Determines how often epochs are saved. Be mindful of your allocated disk limits. Because TMA epochs take so long to complete, you may want to consider lowering this down to 1 during TMA training.\n",
    "* `STEPS_SAVE_FREQ` - Determines how often epochs are saved. Be mindful of your allocated disk limits. Because TMA epochs take so long to complete, you may want to consider lowering this down to 1 during TMA training.\n",
    "* `BATCH_SIZE` - Higher values will increase training speed but require more VRAM. A batch size of 2 will fit on a 16GB GPU during both training stages. \n",
    "* `MULTISPEAKER` - Whether this model is multispeaker.\n",
    "* `SAVE_MODE` - `ITER` to save most recent models, `VAL_LOSS` to save based on validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_EPOCHS = 100 \n",
    "TMA_START_POINT = 32\n",
    "PRETRAINED_MODEL = ''\n",
    "RESUME = False\n",
    "EPOCHS_SAVE_FREQ = 2\n",
    "STEPS_SAVE_FREQ = 300\n",
    "BATCH_SIZE = 4\n",
    "MULTISPEAKER = True\n",
    "SAVE_MODE = 'ITER'\n",
    "\n",
    "if 'styletts_basedir' not in locals():\n",
    "    styletts_basedir = '/root/StyleTTS2'\n",
    "if 'config_path' not in locals():\n",
    "    config_path = os.path.join(styletts_basedir, 'Configs', 'config.yml')\n",
    "\n",
    "import os\n",
    "os.chdir(styletts_basedir)\n",
    "\n",
    "import yaml\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    config['epochs_1st'] = TOTAL_EPOCHS\n",
    "    config['loss_params']['TMA_epoch'] = TMA_START_POINT\n",
    "    config['model_params']['multispeaker'] = MULTISPEAKER\n",
    "    if (PRETRAINED_MODEL != ''):\n",
    "        config['pretrained_model'] = PRETRAINED_MODEL\n",
    "    else:\n",
    "        config['pretrained_model'] = \"\"\n",
    "    config['resume'] = RESUME\n",
    "    config['save_freq'] = EPOCHS_SAVE_FREQ\n",
    "    config['saver_freq_steps'] = STEPS_SAVE_FREQ\n",
    "    config['saver_mode'] = SAVE_MODE\n",
    "    config['batch_size'] = BATCH_SIZE\n",
    "    config['second_stage_load_pretrained'] = False\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "    print(f\"Wrote config options to {config_path}\")\n",
    "\n",
    "!chmod +x ./train_first.sh\n",
    "!./train_first.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second stage (diffusion training/finetuning)\n",
    "[This discussion page](https://github.com/yl4579/StyleTTS2/discussions/81)\n",
    "contains some useful information on finetuning. Some gotchas:\n",
    "* It is basically impossible to finetune on a 16GB GPU--you need 24GB minimum. The original models were trained on 4xA100 (e.g. 160 GB VRAM).\n",
    "* Epochs are zero-indexed in the second stage. This includes `DIFF_BEGIN_EPOCH` and `JOINT_EPOCH`.\n",
    "\n",
    "Configure the settings in the below cell before running.\n",
    "* `IS_FIRST_STAGE` - **!!! Set this if the checkpoint provided is a first stage checkpoint !!!**.\n",
    "* `MAX_LEN` - the maximum length of training samples, in frames (.0125 second/frame; 100 frames = 1.25 seconds, 800 frames = 10 seconds). This will affect VRAM usage and the ability of the model to coherently synthesize longer samples. A `MAX_LEN` below 175 is not recommended. Do not reduce `MAX_LEN` below 100.\n",
    "* `TOTAL_EPOCHS` - total epochs used for second stage training. The paper used 60 epochs on LJSpeech, 40 epochs on VCTK, and 25 epochs on LibriTTS.\n",
    "* `DIFF_BEGIN_EPOCH` - The epoch at which style diffusion training begins. This can be as low as `1` (i.e. second epoch) for large datasets. If you set this above `TOTAL_EPOCHS`, you can disable style vector diffusion, lowering VRAM usage at the cost of output quality. \n",
    "* `JOINT_BEGIN_EPOCH` - The epoch at which SLM adversarial training begins. This cannot be set lower than `DIFF_BEGIN_EPOCH` or an error will occur (SLM adversarial cannot be run without diffusion). SLM adversarial training will increase VRAM usage. A model can be produced without adversarial training, at the expense of output quality.\n",
    "* `BATCH_SIZE` - Higher values will increase training speed but require more VRAM. **Will break if lowered below (NUM_GPUS) * 2.**\n",
    "* `SAVE_FREQ` - Determines how often epochs are saved. Be mindful of your allocated disk limits. Because TMA epochs take so long to complete, you may want to consider lowering this down to 1 during TMA training.\n",
    "* `PRETRAINED_MODEL` - if specified, loads the model and treats as a fresh run on the existing parameters (reset epoch/optimizer). **To make training treat this as a first stage model, the model name must start with `epoch_1st`.** Will be overridden by resume behavior.\n",
    "* `RESUME` - Whether to treat this run as resuming from an existing checkpoint (meaning that epoch numbers, optimizer states, etc. will not be reset). The default resume implementation will automatically select the most recent checkpoint from the current directory.\n",
    "* `MULTISPEAKER` - Whether this model is multispeaker.\n",
    "* `SAVE_MODE` - `ITER` to save most recent models, `VAL_LOSS` to save based on validation loss.\n",
    "* `SLMADV_MIN_LEN` - Minimum length of SLMadv training samples\n",
    "* `SLMADV_MAX_LEN` - Maximum length of SLMadv training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_FIRST_STAGE = True\n",
    "MAX_LEN = 400\n",
    "TOTAL_EPOCHS = 60\n",
    "DIFF_BEGIN_EPOCH = 30\n",
    "JOINT_BEGIN_EPOCH = 50\n",
    "BATCH_SIZE = 32 # 16, 8, 6\n",
    "EPOCHS_SAVE_FREQ = 1\n",
    "STEPS_SAVE_FREQ = 150\n",
    "PRETRAINED_MODEL = 'epoch_1st_pretrained.pth'\n",
    "RESUME = False\n",
    "MULTISPEAKER = True\n",
    "SAVE_MODE = 'ITER'\n",
    "SLMADV_MIN_LEN = 160\n",
    "SLMADV_MAX_LEN = MAX_LEN\n",
    "\n",
    "if 'styletts_basedir' not in locals():\n",
    "    styletts_basedir = '/root/StyleTTS2'\n",
    "if 'config_path' not in locals():\n",
    "    config_path = os.path.join(styletts_basedir, 'Configs', 'config.yml')\n",
    "\n",
    "import re\n",
    "import os\n",
    "os.chdir(styletts_basedir)\n",
    "\n",
    "import yaml\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    config['max_len'] = MAX_LEN\n",
    "    config['epochs_2nd'] = TOTAL_EPOCHS\n",
    "    config['loss_params']['diff_epoch'] = DIFF_BEGIN_EPOCH\n",
    "    config['loss_params']['joint_epoch'] = JOINT_BEGIN_EPOCH\n",
    "    config['batch_size'] = BATCH_SIZE\n",
    "    config['second_stage_load_pretrained'] = not IS_FIRST_STAGE\n",
    "    config['model_params']['multispeaker'] = MULTISPEAKER\n",
    "    if (PRETRAINED_MODEL != ''):\n",
    "        config['pretrained_model'] = PRETRAINED_MODEL\n",
    "    else:\n",
    "        config['pretrained_model'] = \"\"\n",
    "    config['resume'] = RESUME\n",
    "    config['save_freq'] = EPOCHS_SAVE_FREQ\n",
    "    config['saver_mode'] = SAVE_MODE\n",
    "    config['saver_freq_steps'] = STEPS_SAVE_FREQ\n",
    "    config['slmadv_params']['min_len'] = SLMADV_MIN_LEN\n",
    "    config['slmadv_params']['max_len'] = SLMADV_MAX_LEN\n",
    "\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f, default_flow_style=False)\n",
    "    print(f\"Wrote config options to {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./train_second.sh\n",
    "!./train_second.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install restart script (for vast.ai)\n",
    "Useful for vast.ai interruptible instances. Takes the current config as configured above and writes it to `Configs/config_resume.yml` with `config['resume']=True`. Note that this means if you decide to update any config settings above mid-training you should re-run the below cell too.\n",
    "\n",
    "The `onstart.sh` script restarts training as a forked process. The log from a continued training process will not be visible from Jupyter but can be read from train.log (`tail -f train.log`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "if 'styletts_basedir' not in locals():\n",
    "    styletts_basedir = '/root/StyleTTS2'\n",
    "if 'config_path' not in locals():\n",
    "    config_path = os.path.join(styletts_basedir, 'Configs', 'config.yml')\n",
    "config_resume_path = Path(config_path).parent / \"config_resume.yml\"\n",
    "\n",
    "with open(config_path) as f:\n",
    "    config2 = yaml.safe_load(f)\n",
    "    config2['resume'] = True\n",
    "\n",
    "with open(config_resume_path, 'w') as f:\n",
    "    yaml.dump(config2, f, default_flow_style=False)\n",
    "    print(f\"Writing resume config to {config_resume_path}\")\n",
    "\n",
    "!cp onstart.sh /root/onstart.sh\n",
    "!chmod a+rwx /root/onstart.sh /root/StyleTTS2/*.sh\n",
    "print(f\"Onstart script installed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
